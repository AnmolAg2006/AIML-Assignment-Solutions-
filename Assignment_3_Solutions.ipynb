{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fa100ac",
   "metadata": {},
   "source": [
    "# ML Assignment 3: Naive Bayes, Decision Trees, and Ensemble Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12ad5bc",
   "metadata": {},
   "source": [
    "## Task 1 : Theory Questions\n",
    "1. What is the core assumption of Naive Bayes?\n",
    "    Naive Bayes assumes that all features (input variables) are conditionally independent of each other given the class label. This means the presence or absence of one feature does not affect the presence of another, simplifying computation using Bayes’ Theorem.\n",
    "\n",
    "2. Differentiate between GaussianNB, MultinomialNB, and BernoulliNB.\n",
    "\n",
    "    GaussianNB is used when the features follow a normal (Gaussian) distribution and is ideal for continuous data.\n",
    "\n",
    "    MultinomialNB is suitable for discrete data like word counts or frequencies, commonly used in text classification.\n",
    "\n",
    "    BernoulliNB works with binary/boolean features, assuming features are either 0 or 1 (e.g., word presence or absence).\n",
    "\n",
    "3. Why is Naive Bayes considered suitable for high-dimensional data?\n",
    "    Naive Bayes performs well with high-dimensional data because it simplifies computation by assuming feature independence, which avoids the need for modeling complex relationships. Its training is fast and efficient even when the number of features is very large, such as in text classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceffb7c2",
   "metadata": {},
   "source": [
    "## Task 2: Spam Detection using MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0963fb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "# Load SMS Spam Collection Dataset\n",
    "url = \"https://raw.githubusercontent.com/justmarkham/pycon-2016-tutorial/master/data/sms.tsv\"\n",
    "df = pd.read_csv(url, sep='\\t', header=None, names=['label', 'message'])\n",
    "\n",
    "# Encode labels\n",
    "df['label_num'] = df.label.map({'ham':0, 'spam':1})\n",
    "\n",
    "# Vectorization\n",
    "vec = CountVectorizer()\n",
    "X = vec.fit_transform(df['message'])\n",
    "y = df['label_num']\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train MultinomialNB\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af22113",
   "metadata": {},
   "source": [
    "## Task 3: GaussianNB on Iris Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8beab22",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, y_train)\n",
    "gnb_pred = gnb.predict(X_test)\n",
    "\n",
    "lr = LogisticRegression(max_iter=200)\n",
    "lr.fit(X_train, y_train)\n",
    "lr_pred = lr.predict(X_test)\n",
    "\n",
    "print(\"GaussianNB Accuracy:\", accuracy_score(y_test, gnb_pred))\n",
    "print(\"Logistic Regression Accuracy:\", accuracy_score(y_test, lr_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26099a4f",
   "metadata": {},
   "source": [
    "## Task 4 : Conceptual Questions\n",
    "1. What is entropy and information gain?\n",
    "\n",
    "    Entropy measures the impurity or uncertainty in a dataset.\n",
    "\n",
    "    Information Gain is the reduction in entropy after a dataset is split on an attribute—it helps decide the best feature to split on in a decision tree.\n",
    "\n",
    "2. Explain the difference between Gini Index and Entropy.\n",
    "    Both are used to measure impurity, but:\n",
    "\n",
    "    Gini Index focuses on misclassification probability and is faster to compute.\n",
    "\n",
    "    Entropy uses logarithmic calculations and is based on information theory. They often give similar results, but Gini is slightly more efficient.\n",
    "\n",
    "3. How can a decision tree overfit? How can this be avoided?\n",
    "    A decision tree can overfit by growing too deep and capturing noise in the training data.\n",
    "    This can be avoided by pruning, setting a maximum depth, minimum samples per leaf, or using ensemble methods like Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275cc6cd",
   "metadata": {},
   "source": [
    "## Task 5: Decision Tree on Titanic Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75eb998d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import seaborn as sns\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "titanic = sns.load_dataset('titanic')\n",
    "df = titanic[['survived', 'pclass', 'sex', 'age', 'sibsp', 'fare', 'embarked']].dropna()\n",
    "le = LabelEncoder()\n",
    "df['sex'] = le.fit_transform(df['sex'])\n",
    "df['embarked'] = le.fit_transform(df['embarked'])\n",
    "\n",
    "X = df.drop('survived', axis=1)\n",
    "y = df['survived']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "tree = DecisionTreeClassifier(random_state=42)\n",
    "tree.fit(X_train, y_train)\n",
    "y_pred = tree.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plot_tree(tree, feature_names=X.columns, class_names=['Not Survived', 'Survived'], filled=True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06074836",
   "metadata": {},
   "source": [
    "## Task 6: Decision Tree Model Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7b300a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_acc, test_acc = [], []\n",
    "depths = range(1, 20)\n",
    "for d in depths:\n",
    "    model = DecisionTreeClassifier(max_depth=d, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    train_acc.append(model.score(X_train, y_train))\n",
    "    test_acc.append(model.score(X_test, y_test))\n",
    "\n",
    "plt.plot(depths, train_acc, label='Train Accuracy')\n",
    "plt.plot(depths, test_acc, label='Test Accuracy')\n",
    "plt.xlabel('Max Depth')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Overfitting Visualization')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01096032",
   "metadata": {},
   "source": [
    "## Task 7: Conceptual Questions\n",
    "1. What is the difference between Bagging and Boosting?\n",
    "\n",
    "    Bagging builds multiple independent models in parallel using random subsets of data (e.g., Random Forest) to reduce variance.\n",
    "\n",
    "    Boosting builds models sequentially, where each new model focuses on correcting the errors of the previous one to reduce bias.\n",
    "\n",
    "2. How does Random Forest reduce variance?\n",
    "    Random Forest reduces variance by averaging predictions from many decision trees trained on different random subsets of data and features. This aggregation smooths out individual tree errors and prevents overfitting.\n",
    "\n",
    "3. What is the weakness of boosting-based methods?\n",
    "    Boosting methods can overfit noisy data and take longer to train due to their sequential nature. They are also sensitive to outliers since each step tries to fix previous mistakes aggressively.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45192d3",
   "metadata": {},
   "source": [
    "## Task 8: Random Forest vs Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5222c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "rf_pred = rf.predict(X_test)\n",
    "\n",
    "print(\"Random Forest Accuracy:\", accuracy_score(y_test, rf_pred))\n",
    "print(\"Random Forest Precision:\", precision_score(y_test, rf_pred))\n",
    "print(\"Random Forest Recall:\", recall_score(y_test, rf_pred))\n",
    "\n",
    "importances = rf.feature_importances_\n",
    "plt.barh(X.columns, importances)\n",
    "plt.title(\"Feature Importances\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9f3cac",
   "metadata": {},
   "source": [
    "## Task 9: AdaBoost Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebfa061",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "start = time.time()\n",
    "ada = AdaBoostClassifier(random_state=42)\n",
    "ada.fit(X_train, y_train)\n",
    "ada_pred = ada.predict(X_test)\n",
    "end = time.time()\n",
    "\n",
    "print(\"AdaBoost Accuracy:\", accuracy_score(y_test, ada_pred))\n",
    "print(\"F1 Score:\", f1_score(y_test, ada_pred))\n",
    "print(\"Training Time:\", round(end - start, 4), \"seconds\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
